{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "import stanfordnlp\n",
    "import pandas as pd\n",
    "\n",
    "from ProcessArticle import ProcessArticle\n",
    "\n",
    "from WikiXmlHandler import WikiXmlHandler\n",
    "import xml.sax\n",
    "import mwparserfromhell\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from multiprocessing import Pool \n",
    "from multiprocessing import Lock\n",
    "import tqdm \n",
    "\n",
    "# List of lists to single list\n",
    "from itertools import chain\n",
    "\n",
    "# Sending keyword arguments in map\n",
    "from functools import partial\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Use device: cpu\n---\nLoading: tokenize\nWith settings:\n{'model_path': './stanfordnlp_resources/en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: pos\nWith settings:\n{'model_path': './stanfordnlp_resources/en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': './stanfordnlp_resources/en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n---\nLoading: lemma\nWith settings:\n{'model_path': './stanfordnlp_resources/en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nBuilding an attentional Seq2Seq model...\nUsing a Bi-LSTM encoder\nUsing soft attention for LSTM.\nFinetune all embeddings.\n[Running seq2seq lemmatizer with edit classifier]\n---\nLoading: depparse\nWith settings:\n{'model_path': './stanfordnlp_resources/en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': './stanfordnlp_resources/en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\nDone loading processors!\n---\n"
    }
   ],
   "source": [
    "infoboxes = pd.read_csv(\"./saved/infobox-list.csv\")[\"infobox template name\"].values.tolist()\n",
    "folder = \"./data\"\n",
    "files = [folder + \"/\" + x for x in os.listdir(folder)]\n",
    "file = files[0] # process only one file for now\n",
    "\n",
    "out_file = \"./saved/out_nlp_.csv\"\n",
    "\n",
    "nlp = stanfordnlp.Pipeline(models_dir=\"./stanfordnlp_resources/\", use_gpu=False);\n",
    "\n",
    "func = ProcessArticle(infoboxes, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Started processing: ./data/enwiki-latest-pages-articles-multistream1.xml-p10p30302.bz2\nSaved to csv, current number of articles <1000>\nSaved to csv, current number of articles <2000>\nSaved to csv, current number of articles <3000>\nSaved to csv, current number of articles <4000>\nSaved to csv, current number of articles <5000>\nSaved to csv, current number of articles <6000>\nSaved to csv, current number of articles <7000>\nSaved to csv, current number of articles <8000>\nSaved to csv, current number of articles <9000>\nSaved to csv, current number of articles <10000>\nSaved to csv, current number of articles <11000>\nSaved to csv, current number of articles <12000>\nSaved to csv, current number of articles <13000>\nSaved to csv, current number of articles <14000>\nSaved to csv, current number of articles <14967>\nWall time: 2h 14min 6s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "handler = WikiXmlHandler(out_file, func, 1000)\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "print(\"Started processing: {}\".format(file))\n",
    "for line in subprocess.Popen([\"bzcat\"],\n",
    "                        stdin=open(file),\n",
    "                        stdout=subprocess.PIPE).stdout:\n",
    "    try:\n",
    "        parser.feed(line)\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "# making sure that everything was written to the file\n",
    "handler.write_df()"
   ]
  }
 ]
}